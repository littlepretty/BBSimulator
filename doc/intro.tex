\section{Introduction}

%1. Current IO structure
In today's high-performance computing (HPC) systems, application's performance
is no longer only throttled by computation capability, but also perceived I/O rate between
enumous amount of parallel processor cores and petabyte volume of storage equipments.
Current IO architects put IO forwarding nodes in charge of performing IO.
These IO gateways, together with parallel file system (PFS) software (client side), sits
between internal system networks that serving communication between compute nodes
and external system networks that interconnects storage nodes\cite{Ross:IOSystem}.
Applications under this archietecure can expect to achieve
810 GB/s per PFS sustained bandwidth in Trinity's Lustre\cite{TrinitySystem},
but still far from the target of 60 TB/s for exascale computing platform\cite{Shalf:HPCCS:2010}.

%2. Challenge to current architecture
The challenge roots in the missing gap in HPC's memory hierarchy.
The ratio of IO rate of memory on the compute node to the storage disk
is 100 to 10,000 cycles\cite{TrinitySystem}.
Such a gap makes difference because scientific applications on HPC are exposed to
bursty IO patterns\cite{Carns:MSST:2011, Kim:PDSW:2010}.
On one hand, applications have the need to checkpoint perodicall as
defensive IO strategy\cite{Latham:CSD:2012, Naik:ICPPW:2009, Dennis:CUG:2009}
or store intermediate output for subsequent use in visualization;
on the other hand, pushing data from memory to external, parallel file system is
unproductive due to this IO gap.
Even worst, production applications could generate hundreds of GiB to tens of TiB data
in one such IO requesti, reported on Intrepid\cite{Liu:MSST:2012}.
Therefore current IO architecture must make significant change to catch up with
the ever-increasing computational performance and parallelism of HPC system.

%3. The appear of burst buffer
Burst buffer nodes debut as a rescue by utilizing various types of memory,
for example, non-volatile random-access memory (NVRAM) and solid state drive (SSD).
The driver behind is these storage technologies' decreasing cost of bandwidth.
In practice, it can suits up with DataWarp application I/O accelerator\cite{DataWarp}.
\textit{A figure here illustrate the architecture of Burst Buffer}.
%The volume of data read/write may affect the architecture model of burst buffer.
Burst buffer nodes on Trinity is composed of IO nodes and 2 PCIe SSD cards,
connected via totally 16 PCIe 3.0 interfaces.
Alternatively, many researchers proposed to distribute burst buffer 
on multiple layers of the memory hierarchy\cite{Romanus:CORR:15}.
For example, they may be deployed at local computer nodes, board in cabinet or IO nodes.
We may also use burst buffer as intermediate storage system.

%4. Use cases of burst buffer
Regardless of the specific implementation, these nodes essentially augment
the IO stack with a intermediate, productive offloading layer.
For example, an application's latest checkpoing can be pre-staged
before previous job terminates;
or an application can burst its checkpoint to burst buffer
with extremely high speed (4.4-17.8 TB/s on Trinity);
upon termination, application data is also able to drain off
asynchronously to external PFS.
When utilizing burst buffer in this primary scenario (\textit{checkpoint restart}),
bursty application IO operations can thus be aggregated and absorbed into burst buffers.
This makes it possible to shift computations that follows IO bursts to an earlier moment
while burst buffer takes charge of moving potentially TB-level volumes of data.
There are more use cases for burst buffer nodes.
Among them \textit{data cache} could be equally important to enhance the responsiveness
of applications by improving the perceived IO bandwidth\cite{BBUseCase}.
For example, shared object library or read-only configuration files could be
cached on burst buffer nodes;
lists of input files specific to a group of compute nodes allocated to
a particular application could be loaded to burst buffer prior to execution;
Economical solid-state disks as a tier of burst buffer could also be used as
out-of-core complement to insufficient main memory\cite{Romanus:CORR:15},
working place for data analysis (reductions, feature extraction compression etc.)
and visualization\cite{BBUseCase}.

%5. Motivate burst buffer aware scheduler
Given the critical role of burst buffer in future HPC IO system,
we expect user will be actively involved in requesting it for
better their own job's performance.
As a result, it is necessary, or even urgent, to systematically manage
the allocation of these second precious resources (secondary to compute nodes).
This naturally falls into the responsibility of HPC workload scheduler.
Unfortunately, existing schedulers
either have not yet been aware to burst buffer\cite{Moab, other scheduler citation needed},
or just provide very naive allocation policy\cite{SlurmBBGuide}.
This paper tries to bridge two isolated fields of HPC architecture,
the novel burst buffer equipped HPC IO subsystem and
traditional batch job queueing subsystem.
The bridge build upon a 3-phase model that motivated by two of the most
important usage cases of burst buffer:
application checkpoint restart and data cache/pre-fetch for stage in/out.
The benefit of burst buffer nodes will be more than just higher transfer
bandwidth, if it were intelligently allocated by scheduler.
For example, the first case could speedups the \textit{running phase} of
user's application by acutely absorb the bursty checkpoint-purpose IO request;
the second case reduce the application's waiting time via
contracted input/output stage in the execution pipeline of application series.
%6. Contribution summary and paper structure
Our contributions in this paper are summarized as follows:
\begin{enumerate}
        \item Explore how HPC workload scheduler allocates burst buffer resources.
                We propose a 3-phase application model tailored the typical
                usage scenarios of burst buffer, that is, checkpoint restart,
                data file/cache stage in and stage out.
        \item On the basis of 3-phase job model, we present Cerberus,
                a burst buffer aware HPC workload scheduler.
                Dividing the lifetime of user application to different phases,
                Cerberus makes it possible to conquer the scheduling goal separately.
        \item We suggest several optimizing goals for each phases.
                Though optimal scheduling problem in each phase is NP-hard,
                dynamic programming with memoization could give precise solutions
                in practice.
\end{enumerate}

The next section begins elaborating the 3-phase model (section~\ref{Sec:Model}),
after which Cerberus is introduced in section~\ref{Sec:Scheduler}.
The details of formulating and solving scheduling problems with
dynamic programming at each job phase are also
enumerated in section~\ref{Sec:Scheduler}.
Starting from section~\ref{Sec:Experiments}, we validate Cerberus
by simulating the full Trinity supercomputing platform, featured with
burst buffer hardware.
Related works are discussed in section~\ref{Sec:RelatedWorks}.
We conclude this paper and list possible future works in section~\ref{Sec:Conclusion}.





