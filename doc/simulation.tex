\section{Simulation Results}
We consider simulating the full Trinity super computer.
The number of compute nodes on Trinity is about 18936, 9436 Intel Haswell nodes and at least 9500 Intel Xeon Phi nodes.
There are 16 cores on each processor, thus totally 302976 cores.
In the following experiments, we compare two identical system except that IO nodes are replaced by the same number of burst buffer nodes.
Burst buffer nodes provide total capacity of 0.4 PB PCIe SSD intermediate storage.
Sequencial read/write between burst buffer and compute nodes is 5.7 GB/s.
Bandwidth between CPU node and IO node is set to 0.9 GB/s.
Job trace is from ANL's Blue Gene Intrepid system from January to September 2009.
Each log entry contains information like jobs' submission time, running time, number of cores user requested and so on. 
It is truncated so that only the first 1000 jobs are used in simulation.
We patched 3 fields to each job's log entry: the amount of input data $data\_in$, the amount of written data during checkpointing $data\_run$ and the amount of outputing data $data\_out$.
We assume they follows uniform distribution with low boundary 1000 GB and high boundary 10000 GB.

Following sections discuss or answer 3 key questions.
Will Cerberus improve application performance by utilizing burst buffer nodes?
Can Cerberus with optimization improve application performance?
Will job demand on burst buffer effect Cerberus?

\subsection{Cerberus vs. 1-Phase Batch Scheduler}
In this section, we demonstrate that by utilizing burst buffer nodes, job scheduler could improve the applicaitons' performance.
Figure x compares CDF of the response time of 1000 jobs.

\subsection{Cerberus vs. Cerberus with Optimization}
If we consider optimizing either burst buffer's data throughput or the parallelism across jobs, dynamic programming based job scheduler can further reduce jobs' wait time.
We plot in Figure x the resulting response time of three different scheduler regarding how they handle jobs in their queues(input queue, run queue, and output queue).
The first scheduler uses naive FCFS policy.
Whoever at the front of queue are considered favorably.
The second and third scheduler treat jobs in run queue identically.
They choose jobs according to the optimization solution given by \ref{Equ:MaxProductRecursion}
However, they treat jobs in the input queue and output queue differently.
The second scheduler will select these jobs in its queue so that volumn of transferred data is maximized.
The third scheduler tries to optimize the number of schedulable jobs by using \ref{Equ:MaxTaskNumberRecursion}.

\subsection{Cerberus vs. Demand Granularity}
In this section we validate our 3-phase model.
Applications are benifited when scheduler dividing jobs into 3 separated phases.
Scheduling are based on correponding burst buffer demand in each phase.
This suggests that user should provide burst buffer demand as granular as possible.
In Figure x, we plot 3 different scheduling results by 3 FCFS scheduler.
Jobs in the first case are modeled as just 1 phase because user just provides a general burst buffer demand throughout entire application life time.
We assume this demand is the $\max \{data\_in, data\_out, data\_run\}$.
This is the traditional scheduling scheme except job has additional burst buffer demand.
Jobs in the second and third cases have 3 phases and are scheduled by Cerberus.
However, users provided all the burst buffer demand in 3 phases at the third case.




